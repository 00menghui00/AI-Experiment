# 使用deepspeed分布式训练最终得到的模型结果：

### 文件分类总览

| 类别 | 文件名 | 来源 & 状态 | 是否可人为修改？ |
| :--- | :--- | :--- | :--- |
| **1. 模型权重与状态** | `model-*.safetensors` / `*.pt` | **训练生成** (核心产物) | **否** (由训练过程生成和更新) |
| | `model.safetensors.index.json` | **训练生成** | **否** (自动生成以索引权重) |
| **2. 模型架构与配置** | `config.json` | **预训练自带** / 开发者定义 | **是** (高级用户可修改超参数) |
| | `generation_config.json` | 开发者定义 / **预训练自带** | **是** (最常修改以调整生成行为) |
| **3. 分词器 (Tokenizer)** | `tokenizer.json` | **预训练自带** / 训练生成 | **否** (通常由工具生成) |
| | `tokenizer_config.json` | **预训练自带** / 开发者定义 | **是** (可修改分词器行为) |
| | `vocab.json` | **预训练自带** / 训练生成 | **否** (词汇表，由训练确定) |
| | `merges.txt` | **预训练自带** / 训练生成 | **否** (BPE合并规则，由训练确定) |
| | `special_tokens_map.json` | **预训练自带** / 开发者定义 | **是** (可定义或修改特殊标记) |
| | `added_tokens.json` | **训练生成** / 开发者定义 | **是** (可手动添加新词汇) |
| | `chat_template.jinja` | 开发者定义 / **预训练自带** | **是** (可自定义对话格式) |
| **4. 训练过程与状态** | `training_args.bin` | **训练生成** | **否** (记录启动参数) |
| | `trainer_state.json` | **训练生成** | **否** (记录训练进度) |
| | `scheduler.pt` | **训练生成** | **否** (记录学习率调度器状态) |
| | `*optim_states.pt` | **训练生成** | **否** (记录优化器状态) |
| | `rng_state_*.pth` | **训练生成** | **否** (记录随机数状态) |
| | `latest` | **训练生成** | **否** (指向最新的检查点) |
| | `zero_to_fp32.py` | 开发者定义 / 框架提供 | **是** (这是一个工具脚本) |

---

### 各类文件详细解析

#### 1. 模型权重与状态 (模型的核心)

*   **`model-*.safetensors` 或 `mp_rank_00_model_states.pt`**:
    *   **这是什么**: 模型权重文件，即模型的“大脑”，包含了所有经过训练学习到的参数。`.safetensors` 是更安全、更快速的新格式。
    *   **来源**: **训练过程中生成**。这是模型训练的最终产物。
    *   **可修改?**: **否**。这些是二进制文件，直接修改会损坏模型。
    *   **模型训练完后得到的.pt文件是什么**: 就是指这类文件。它们是部署和推理时必须加载的核心文件。

*   **`model.safetensors.index.json`**:
    *   **这是什么**: 权重索引文件。当模型太大被分割成多个 `.safetensors` 文件时，这个文件充当“地图”，告诉程序去哪个文件里找哪个参数。
    *   **来源**: **训练过程中生成**。
    *   **可修改?**: **否**。它与权重文件严格对应。

#### 2. 模型架构与配置 (模型的蓝图和行为指南)

*   **`config.json`**:
    *   **这是什么**: 模型的“蓝图”或“身份证”，定义了模型的架构，如层数、头数、隐藏层大小等。
    *   **来源**: **预训练模型自带**。微调时，开发者可以基于此进行修改。
    *   **可修改?**: **是**。高级用户可以调整这些超参数来改变模型结构，但这需要重新训练。

*   **`generation_config.json`**:
    *   **这是什么**: 模型的“行为指南”，控制模型在推理时如何生成文本，如温度（`temperature`）、采样方式（`top_k`, `top_p`）等。
    *   **来源**: **预训练模型自带**，但通常是**开发者定义**或微调以获得最佳效果。
    *   **可修改?**: **是**。这是最常被修改的文件，用来调整生成文本的风格（例如，更有创意或更保守）。

#### 3. 分词器 (Tokenizer) (模型的翻译官)

*   **`tokenizer.json`, `vocab.json`, `merges.txt`**:
    *   **这是什么**: 分词器的核心组件。`vocab.json` 是词汇表，`merges.txt` 是BPE合并规则，`tokenizer.json` 是一个包含所有信息的综合文件。
    *   **来源**: **预训练模型自带**。如果在训练中增加了新词汇，这些文件会被重新生成。
    *   **可修改?**: **否**。它们是根据训练数据生成的，手动修改会导致分词错误。

*   **`tokenizer_config.json`, `special_tokens_map.json`, `added_tokens.json`**:
    *   **这是什么**: 分词器的配置文件。`tokenizer_config.json` 定义总体行为，`special_tokens_map.json` 映射特殊标记（如 `[PAD]`），`added_tokens.json` 记录微调时新增的词。
    *   **来源**: **预训练模型自带**，但**开发者**在微调时经常会**定义或修改**它们。
    *   **可修改?**: **是**。例如，你可以通过修改 `special_tokens_map.json` 来改变特殊标记。

*   **`chat_template.jinja`**:
    *   **这是什么**: 对话格式模板。它定义了如何将多轮对话（用户、助手）格式化成模型能理解的单一字符串。
    *   **来源**: **预训练模型自带**，但**开发者**可以自定义以适应特定任务。
    *   **可修改?**: **是**。你可以修改它来改变模型的对话输入格式。

#### 4. 训练过程与状态 (用于恢复训练的“记忆”)

这些文件对于从中断点恢复训练至关重要，但在最终部署推理时通常不是必需的。

*   **`training_args.bin`**: 记录启动训练时使用的所有参数。
*   **`trainer_state.json`**: 记录训练的全局状态，如当前步数、轮次等。
*   **`scheduler.pt`**: 记录学习率调度器的状态。
*   **`*optim_states.pt`**: 记录优化器（如Adam）的状态，这在分布式训练中会被分割成多个文件。
*   **`rng_state_*.pth`**: 记录每个GPU上随机数生成器的状态，以保证训练的可复现性。
*   **`latest`**: 一个简单的文本文件，内容是最新检查点目录的名称。
*   **`zero_to_fp32.py`**: 一个工具脚本，用于在使用了ZeRO优化的训练后，将模型权重从低精度转换回标准的32位浮点数。

*   **来源**: 全部是**训练过程中自动生成**的。
*   **可修改?**: **否**。这些是训练框架的状态记录，手动修改会破坏检查点，导致无法正确恢复训练。
