
# DeepSeek-V3 单卡 A100 对话系统部署流程说明

## 1️⃣ 前提条件

| 条件     | 说明                                 |
| ------ | ---------------------------------- |
| GPU    | NVIDIA A100 40GB                   |
| 系统     | Ubuntu 20.04/22.04                 |
| Python | 3.10+                              |
| 模型文件   | `inference/` 文件夹 + 权重文件（fp16 或量化版） |
| 网络     | 可访问外网下载依赖或模型权重                     |
| 端口     | 用于 Web 服务（如 8000）                  |

---

## 2️⃣ 环境准备

1. 创建虚拟环境：

```bash
conda create -n deepseek python=3.10 -y
conda activate deepseek
```

2. 安装 PyTorch 和推理依赖：

```bash
# 安装 GPU 版本 PyTorch
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
# 安装其他推理依赖
pip install transformers accelerate flask uvicorn
# 可选：vLLM/TensorRT-LLM 用于高性能推理
pip install vllm
```

---

## 3️⃣ 准备模型

1. 下载 DeepSeek-V3 推理文件夹（`inference/`）
2. 下载模型权重文件（fp16 或 bf16 版本）到 `inference/` 目录
3. 可选：量化模型以减少显存占用

```python
from model import DeepSeekModel
import torch

# 加载配置
config = "configs/config_671B.json"
model = DeepSeekModel(config)
model.load_state_dict(torch.load("checkpoint.pt"))
# 量化为半精度
model = model.half()  
model.to("cuda")
model.eval()
```

> 量化选择：
>
> * FP16 / BF16：显存占用减半
> * INT8 / FP8：进一步节省显存（适合大模型）
> * INT4：显存最省，但可能略有精度损失

---

## 4️⃣ 构建推理 API 服务

### 4.1 FastAPI 示例

```python
from fastapi import FastAPI
from pydantic import BaseModel
from model import DeepSeekModel
import torch

app = FastAPI()

# 加载模型
config = "configs/config_671B.json"
model = DeepSeekModel(config)
model.load_state_dict(torch.load("checkpoint.pt"))
model.half().to("cuda")  # FP16
model.eval()

class Prompt(BaseModel):
    text: str

@app.post("/generate")
def generate(prompt: Prompt):
    with torch.no_grad():
        output = model.generate(prompt.text, max_new_tokens=200)
    return {"response": output}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

* 说明：

  * `max_new_tokens` 控制每次生成长度
  * `torch.no_grad()` 防止显存被梯度占用
  * `half()` + `.to("cuda")` 减少显存占用

---

### 4.2 启动服务

```bash
uvicorn app:app --host 0.0.0.0 --port 8000
```

* 服务器访问：`http://服务器IP:8000/docs` 可查看 FastAPI 自动生成的接口文档

---

## 5️⃣ 构建前端对话网页

```html
<!DOCTYPE html>
<html>
<head>
  <title>DeepSeek-V3 Chat</title>
</head>
<body>
  <h1>对话系统</h1>
  <input id="userInput" type="text" placeholder="输入问题...">
  <button onclick="sendPrompt()">发送</button>
  <div id="response"></div>

  <script>
    async function sendPrompt() {
      const prompt = document.getElementById("userInput").value;
      const res = await fetch("http://服务器IP:8000/generate", {
        method: "POST",
        headers: {"Content-Type": "application/json"},
        body: JSON.stringify({text: prompt})
      });
      const data = await res.json();
      document.getElementById("response").innerText = data.response;
    }
  </script>
</body>
</html>
```

* 用户在网页输入问题，后台 FastAPI 调用模型生成输出
* 支持单用户或局域网多用户访问

---

## 6️⃣ 性能优化（单卡 A100）

1. **量化**：

   * FP16/FP8/INT8/INT4，减小显存占用
2. **KV cache**：

   * 对连续对话上下文启用缓存，提高生成速度
3. **异步请求处理**：

   * FastAPI 异步 + GPU queue，避免阻塞
4. **大模型注意事项**：

   * 30B+ 模型量化后单卡勉强可用
   * 65B+ 模型需要多卡或 pipeline parallel

---

## 7️⃣ 总结流程

1. 获取 `inference/` 文件夹
2. 下载模型权重（fp16/量化版）
3. 准备 A100 服务器环境（Python + PyTorch + 推理依赖）
4. 可选：量化模型以减少显存占用
5. 编写 FastAPI/Flask 接口，加载模型并暴露 `/generate` 接口
6. 启动服务，确认 GPU 可用
7. 构建前端网页，调用 API 实现对话系统
8. 可选性能优化：量化 + KV cache + 异步队列


