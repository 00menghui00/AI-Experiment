- dpo:https://github.com/QwenLM/Qwen3-Coder/tree/main/finetuning/dpo
- sft:https://github.com/QwenLM/Qwen3-Coder/tree/main/finetuning/sft

# DPO：

DPO的核心逻辑是：**不再通过强化学习的复杂路径，而是直接利用一个简单的分类损失，来最大化模型生成“偏好”回答（chosen）相对于“不偏好”回答（rejected）的概率。**

这个核心逻辑在脚本中主要通过以下**三个关键点**来体现：

---

### 1. **数据的特定格式：偏好对 (`chosen` vs. `rejected`)**

这是 DPO 与 SFT 最根本的区别，也是其核心逻辑的**起点**。

*   **SFT 的数据**: `(指令, 唯一正确答案)`
*   **DPO 的数据**: `(指令, 偏好的答案_chosen, 不偏好的答案_rejected)`

脚本中处理数据的部分明确体现了这一点：
```python
# 在 process_sample 函数中，返回的字典包含了三个关键字段
example = {
    'prompt': ...,
    'chosen': row["chosen"],
    'rejected': row["rejected"],
}
```
`DPOTrainer` 被设计为专门接收这种包含 `prompt`, `chosen`, `rejected` 三元组的数据。这个数据结构本身就定义了 DPO 的任务：**学习一种“比较”和“选择”的能力，而非 SFT 的“模仿”能力。**

---

### 2. **双模型架构：策略模型 (`model`) vs. 参考模型 (`ref_model`)**

这是 DPO 算法的**核心计算框架**。脚本中明确加载了两个模型实例：

```python
model = transformers.AutoModelForCausalLM.from_pretrained(...)
ref_model = transformers.AutoModelForCausalLM.from_pretrained(...)

trainer = DPOTrainer(
    model,
    ref_model=ref_model,
    ...
)
```

*   **`model` (策略模型, Policy Model, π_θ)**: 这是我们要训练和优化的模型。它的权重会在每个训练步骤中被更新。
*   **`ref_model` (参考模型, Reference Model, π_ref)**: 这是一个**权重固定不变**的基准模型。它通常就是微调开始前的原始预训练模型。

**核心逻辑的体现**:
DPO 的损失函数本质上是在做一个“差值比较”。`DPOTrainer` 在内部会执行以下计算（伪代码）：

1.  **计算策略模型的概率差**:
    *   `policy_chosen_logps` = 用 `model` 计算 `chosen` 回答的对数概率。
    *   `policy_rejected_logps` = 用 `model` 计算 `rejected` 回答的对数概率。
    *   `policy_logratios` = `policy_chosen_logps` - `policy_rejected_logps`

2.  **计算参考模型的概率差**:
    *   `ref_chosen_logps` = 用 `ref_model` 计算 `chosen` 回答的对数概率。
    *   `ref_rejected_logps` = 用 `ref_model` 计算 `rejected` 回答的对数概率。
    *   `ref_logratios` = `ref_chosen_logps` - `ref_rejected_logps`

`ref_model` 的作用是提供一个**稳定的锚点**。DPO 的目标不仅仅是让 `model` 更喜欢 `chosen`，更是要让 `model` 相对于 `ref_model` 的**偏好提升程度**最大化。这可以有效防止模型在学习偏好的过程中，忘记其原有的语言知识，即所谓的“模式崩溃”（Mode Collapse）。

---

### 3. **隐式的奖励与损失函数 (DPO Loss)**

这是 DPO 核心逻辑的**数学实现**，也是 `DPOTrainer` 内部最神秘但最关键的部分。

DPO 巧妙地证明了，传统的 RLHF 中“最大化奖励”的目标，可以等价地转换为一个简单的、可以直接优化的分类损失。

`DPOTrainer` 内部计算的损失函数大致如下：

`loss = -log_sigmoid(β * (policy_logratios - ref_logratios))`

让我们来拆解这个公式：

*   **`policy_logratios - ref_logratios`**: 这就是模型 `model` 相对于 `ref_model` 的“偏好提升度”。我们希望这个值越大越好（即 `model` 对 `chosen` 的偏好远超 `ref_model`）。
*   **`β` (beta)**: 这是一个超参数（在 `DPOConfig` 中设置），它控制了我们对参考模型的偏离程度的重视程度。`β` 越小，模型可以更大胆地学习新的偏好；`β` 越大，模型就越被“束缚”在参考模型附近，防止其过度改变。
*   **`log_sigmoid(...)`**: 这是一个类似于二元交叉熵的损失函数。
    *   当括号内的值（偏好提升度）很大时，`sigmoid` 的结果趋近于 1，`log(1)` 趋近于 0，损失很小。这很好，说明模型已经学会了偏好。
    *   当括号内的值很小时（甚至为负），`sigmoid` 的结果趋近于 0，`log(0)` 趋近于负无穷，损失变得非常大。这会产生一个强大的梯度信号，迫使模型去调整权重，以提高对 `chosen` 的偏好。

**总结：DPO 的核心逻辑**

综合以上三点，DPO 在该脚本中的核心逻辑可以总结为：

**`DPOTrainer` 接收包含 `(prompt, chosen, rejected)` 的偏好数据，并利用一个可训练的 `model` 和一个固定的 `ref_model`，通过一个巧妙的 `log_sigmoid` 损失函数，直接将“让模型更喜欢 `chosen` 而不是 `rejected`”这一目标，转化为了一个可以通过梯度下降直接优化的、类似分类任务的数学问题，从而高效、稳定地将人类偏好对齐到大模型中。**
